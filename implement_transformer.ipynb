{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a880972-e0ba-4b47-8b37-7567e5a84d31",
   "metadata": {},
   "source": [
    "# Transformer TP\n",
    "In this TP we will implement the transformer model (an architecture based on LLama2). The squeleton of the code is provided and you should complete it (mostly self attention mechanism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24473635-9661-44ad-be96-dd9635e9fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0207 20:24:51.520000 4510 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from time import gmtime, strftime\n",
    "import math\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# neural network utilities\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset an tokenization\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from rope_embedding import RoPE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73434fce-7c28-4395-b25e-919797eb153d",
   "metadata": {},
   "source": [
    "## Self Attention with no mask\n",
    "\n",
    "Let consider $Q, K, V \\in R^{B\\times H \\times L \\times N}$ for each batch element (B) for each heads (H) we want to compute $$A = \\frac{Q_{bh}.K_{bh}^{t}}{\\sqrt{N}}$$\n",
    "\n",
    "Attention weights are defined by $$ \n",
    "softmax(A_i) = \\begin{pmatrix}\n",
    "\\frac{e^{A_{i1}}}{\\sum\\limits_{j=1}^L  e^{A_{ij}}} & \n",
    "\\frac{e^{A_{i2}}}{\\sum\\limits_{j=1}^L  e^{A_{ij}}} & \n",
    "\\dots &\n",
    "\\frac{e^{A_{iL}}}{\\sum\\limits_{j=1}^L  e^{A_{ij}}}\n",
    "\\end{pmatrix}$$ \n",
    "And $$ \n",
    "Softmax(A) = \\begin{pmatrix}\n",
    "\\frac{e^{A_{11}}}{\\sum\\limits_{j=1}^L  e^{A_{1j}}} & \n",
    "\\frac{e^{A_{12}}}{\\sum\\limits_{j=1}^L  e^{A_{1j}}} & \n",
    "\\dots &\n",
    "\\frac{e^{A_{1L}}}{\\sum\\limits_{j=1}^L  e^{A_{1j}}} \\\\\n",
    "\\frac{e^{A_{21}}}{\\sum\\limits_{j=1}^L  e^{A_{2j}}} & \n",
    "\\frac{e^{A_{22}}}{\\sum\\limits_{j=1}^L  e^{A_{2j}}} & \n",
    "\\dots &\n",
    "\\frac{e^{A_{2l}}}{\\sum\\limits_{j=1}^L  e^{A_{2j}}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{e^{A_{L1}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}} & \n",
    "\\frac{e^{A_{L2}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}} & \n",
    "\\dots &\n",
    "\\frac{e^{A_{LL}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}}\n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "\n",
    "Notice that the function softmax can be used to compute this matrix [`torch.softmax`](https://docs.pytorch.org/docs/stable/generated/torch.softmax.html#torch.softmax).\n",
    "\n",
    "\n",
    "## Masked Attention\n",
    "In many cases, certains tokens will not compute attention with later tokens (padding or decoding approaches). Typically in decoder only architectures we would have the attention weights matrix only considering lower diagonal values :\n",
    "\n",
    "$$ \n",
    "Softmax(A) = \\begin{pmatrix}\n",
    "\\frac{e^{A_{11}}}{\\sum\\limits_{j=1}^1  e^{A_{1j}}} & \n",
    "0 & \n",
    "\\dots & 0 &\n",
    "0 \\\\\n",
    "\\frac{e^{A_{21}}}{\\sum\\limits_{j=1}^2  e^{A_{2j}}} & \n",
    "\\frac{e^{A_{22}}}{\\sum\\limits_{j=1}^2  e^{A_{2j}}} & \n",
    "\\dots & 0 &\n",
    "0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{e^{A_{{L-1}1}}}{\\sum\\limits_{j=1}^{L-1}  e^{A_{L-1j}}} & \n",
    "\\frac{e^{A_{{L-1}2}}}{\\sum\\limits_{j=1}^{L-1}  e^{A_{L-1j}}} & \n",
    "\\dots &\n",
    "\\frac{e^{A_{L-1L-1}}}{\\sum\\limits_{j=1}^{L-1}  e^{A_{L-1j}}}& 0 \\\\\n",
    "\\frac{e^{A_{L1}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}} & \n",
    "\\frac{e^{A_{L2}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}} & \n",
    "\\dots &\n",
    "\\frac{e^{A_{LL-1}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}} & \n",
    "\\frac{e^{A_{LL}}}{\\sum\\limits_{j=1}^L  e^{A_{Lj}}} \\\\\n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "A simple implementation would consist in setting the upper diagonal of A to $-\\infty$ and apply the softmax. We will thus consider a matrix name $M$ (or `mask` in the code) having upper diagonal element to $-\\infty$ : \n",
    "$$ M = \n",
    "\\begin{pmatrix}\n",
    "0& \n",
    "-\\infty & \n",
    "\\dots & -\\infty&\n",
    "-\\infty \\\\\n",
    "0 & \n",
    "0 & \n",
    "\\dots & -\\infty &\n",
    "-\\infty \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \n",
    "0 & \n",
    "\\dots &\n",
    "0& -\\infty \\\\\n",
    "0 & \n",
    "0 & \n",
    "\\dots &\n",
    "0 & \n",
    "0\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And apply softmax on $A + M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf3ecaa-8ee7-4940-a225-81f8e96a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(\n",
    "    q : torch.FloatTensor,\n",
    "    k : torch.FloatTensor,\n",
    "    v : torch.FloatTensor,\n",
    "    mask : torch.FloatTensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Given Q,K,V and the attention mask compute cross attention\n",
    "\n",
    "    The shape of the matrix are the following:\n",
    "    B: The batch size (number of examples)\n",
    "    Q: The size of the decoder sequence\n",
    "    K: The size of the encoder sequence\n",
    "    H: Then number of attention heads\n",
    "    N: The embedding size \n",
    "    \n",
    "    Parameters:\n",
    "        q : torch.FloatTensor\n",
    "            The query matrix of shape BxHxQxN\n",
    "        k : torch.FloatTensor\n",
    "            The key matrix of shape BxHxKxN\n",
    "        v : torch.FloatTensor\n",
    "            The value matrix of shape BxHxKxN\n",
    "        mask : torch.FloatTensor\n",
    "            The attention mask of shape BxQxK,\n",
    "            the masked elements are set to -inf \n",
    "            else elements are set to 0\n",
    "    Return : (torch.Tensor, torch.Tensor)\n",
    "        return two tensor the first containing \n",
    "        the attention weights (QK^t) and the second \n",
    "        the result of the attention\n",
    "    \"\"\"\n",
    "    # Get embedding dimension\n",
    "    N = q.size(-1)\n",
    "\n",
    "    # Step 1: Compute scaled dot-product attention scores\n",
    "    # (B, H, Q, N) x (B, H, N, K) → (B, H, Q, K)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(N)\n",
    "\n",
    "    # Step 2: Apply attention mask\n",
    "    # mask shape: (B, Q, K) → broadcast to (B, 1, Q, K)\n",
    "    scores = scores + mask.unsqueeze(1)\n",
    "\n",
    "    # Step 3: Softmax over the last dimension (K)\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 4: Compute weighted sum of values\n",
    "    # (B, H, Q, K) x (B, H, K, N) → (B, H, Q, N)\n",
    "    attention_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "    return attention_weights, attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2849459-0fa5-4508-9052-c641e8fd3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = (torch.ones(1, 1, 3, 4), torch.ones(1, 1, 3, 4), torch.ones(1, 1, 3, 4))\n",
    "mask = torch.triu(-q.new_ones(1, 3, 3) *  torch.inf, diagonal =  1)\n",
    "attention_weigths, attention_output = multi_head_attention(q, k, v, mask)\n",
    "\n",
    "assert(torch.all(attention_weigths == torch.Tensor([\n",
    "          [[[1., 0., 0.],\n",
    "            [1/2, 1/2, 0.],\n",
    "            [1/3, 1/3, 1/3]]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d27a7a",
   "metadata": {},
   "source": [
    "##  Create the attention module\n",
    "\n",
    "In pytorch module are block of neural networks that inherit from the class `nn.Module`.  In the following you willbe asked to complete the attention code. The attention module compute the Query $Q$, the Keys $K$ and Values $V$ for each heads and compute attention. The forward methods is the method implementing these operations. In this lab we use a mult-head attention, meaning that we compute attention for each heads, that can be done with a for loop or using batched operation (it is up to you). Also as we follow the Llama 2 architecture, we use for positional embeding the RoPe positonal encoding that is applied on the two matrix $Q$ and $K$. The forward function should compute for an input $X \\in \\mathbb{R}^{L \\times N}$ (in the decoder only case) the following:\n",
    "\n",
    "1. $Q_h = RoPE(W^q_{h}X^{\\intercal})$ (for all heads h) with $W^q_h \\in \\mathbb{R}^{N \\times N}$\n",
    "2. $K_h = RoPE(W^k_{h}X^{\\intercal})$ (for all heads h)\n",
    "3. $V_h = W^v_{h}X^{\\intercal}$ (for all heads h)\n",
    "4. $A_h =  Attention(Q_h, K_h, V_h)$ (for all heads h)\n",
    "5. $A$ = [A_1, A_2, \\dots A_H] (the concatenation along the last dimension)\n",
    "6. $O = W^oA^{\\intercal}$ with $w^{o} \\in \\mathbb{R}^{N \\times NH}$ (NH being the scalar multplication of N and H)\n",
    "\n",
    "Notice that Q, K and V can be computed without a loop using $W_{q}$ (respectively for $W_k$ and $W_v$) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7564526-ad70-4764-86d2-a0984af9223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttentionModule(nn.Module):\n",
    "    \"\"\" A cross/self attention pytorch module.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_heads=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wq, self.Wk, self.Wv  =\\\n",
    "            (nn.Linear(input_dim, output_dim * num_heads) for _ in range(3))\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.Wo = nn.Linear(output_dim * num_heads, input_dim)\n",
    "        self.rope_func = RoPE(output_dim)\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            x : torch.Tensor,\n",
    "            attention_mask : torch.BoolTensor = None, \n",
    "            y : torch.Tensor = None,\n",
    "            decoder_mask = True,\n",
    "            k_cache = None,\n",
    "            v_cache = None,\n",
    "            start_cache = 0\n",
    "        ):\n",
    "        ''' \n",
    "            Parameters:\n",
    "                x : torch.Tensor\n",
    "                    The input of the Attention module\n",
    "                    used at least for K, V computation\n",
    "                    (for Q if decoder only)\n",
    "                attention_mask : torch.BoolTensor\n",
    "                    The mask for attention\n",
    "                y : torch.Tensor or None\n",
    "                    The query input in the case of\n",
    "                    cross-attention\n",
    "        '''\n",
    "\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        if y is None:\n",
    "            y = x\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.Wq(y)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        # Reshape to multi-head format\n",
    "        Q = Q.view(B, L, self.num_heads, self.output_dim).transpose(1, 2)\n",
    "        K = K.view(B, L, self.num_heads, self.output_dim).transpose(1, 2)\n",
    "        V = V.view(B, L, self.num_heads, self.output_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply RoPE to Q and K\n",
    "        Q = self.rope_func(Q, start_cache)\n",
    "        K = self.rope_func(K, start_cache)\n",
    "\n",
    "        # Decoder causal mask\n",
    "        if decoder_mask and attention_mask is None:\n",
    "            attention_mask = torch.triu(\n",
    "                torch.full((L, L), float(\"-inf\"), device=x.device),\n",
    "                diagonal=1\n",
    "            ).unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "        # Attention\n",
    "        attention_weigths, attention_output = multi_head_attention(\n",
    "            Q, K, V, attention_mask\n",
    "        )\n",
    "\n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(\n",
    "            B, L, self.num_heads * self.output_dim\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        output = self.Wo(attention_output)\n",
    "\n",
    "        return attention_weigths, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c085a7a",
   "metadata": {},
   "source": [
    "## Trasnsformer FeedForward\n",
    "\n",
    "The FeedForward Network is defined using Llama 2 architecture as following for $X \\in \\mathbb{R^{L\\times N}}$ :\n",
    "\n",
    "\n",
    "1. $G = SiLU(W^gX^\\intercal)$ with $W^g \\in \\mathbb{R}^{M \\times N}$ (see SiLU in pytorch [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html))\n",
    "2. $U = W^u X^\\intercal$ with $W^u  \\in \\mathbb{R}^{M \\times N}$\n",
    "3. $I = G \\odot I$ (Hadamard multiplication)\n",
    "4. $O = W^{o} I^{\\intercal}$ with $W_{o} \\in \\mathbb{R}^{N \\times M}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c95d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(embed_size, intermediate_size)\n",
    "        self.up_proj = nn.Linear(embed_size, intermediate_size)\n",
    "        self.down_proj = nn.Linear(intermediate_size, embed_size)\n",
    "        self.gate_func = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        G = self.gate_func(self.gate_proj(x))\n",
    "        U = self.up_proj(x)\n",
    "        I = G * U\n",
    "        O = self.down_proj(I)\n",
    "        return O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd9e1c",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "We can now create the block of the \n",
    "\n",
    "Let consider $x_0 \\in \\mathbb{R}^{B \\times L \\times N}$, the decoder block is given by : \n",
    "\n",
    "* $x_1 = LN_1(x_0)$ (apply layer norm)\n",
    "* $x_2 = Attention(x_1)$ (apply attention)\n",
    "* $x_3 = x_0 + x_2$ (adding residual)\n",
    "* $x_4 = LN_2(x_3)$\n",
    "* $y = FF(x_4)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e08611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, intermediate_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention_module =\\\n",
    "            RoPEAttentionModule(embed_size, embed_size, num_heads=num_heads)\n",
    "        self.feed_forward = TransformerFeedForward(embed_size, intermediate_size)\n",
    "        \n",
    "        self.attention_layer_norm = nn.RMSNorm(embed_size)\n",
    "        self.feed_forward_layer_norm = nn.RMSNorm(embed_size)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        attention_mask = None,\n",
    "        k_cache=None,\n",
    "        v_cache=None,\n",
    "        start_cache=0,\n",
    "        layer=0,\n",
    "    ):\n",
    "        \n",
    "        x_norm = self.attention_layer_norm(x)\n",
    "        _, attn_out = self.attention_module(\n",
    "            x=x_norm,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_mask=True,\n",
    "            k_cache=k_cache,\n",
    "            v_cache=v_cache,\n",
    "            start_cache=start_cache\n",
    "        )\n",
    "        x_res = x + attn_out\n",
    "        \n",
    "        x_ffn_norm = self.feed_forward_layer_norm(x_res)\n",
    "        ff_out = self.feed_forward(x_ffn_norm)\n",
    "        \n",
    "        output = x_res + ff_out\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eed92e",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "The model contain a stack of block on input embedding, it applies the different transformation and should return both the output embeddings and the logits (size of the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8307f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embed_size, intermediate_size, num_heads, hidden_layers=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.nh = num_heads\n",
    "        self.nhl = hidden_layers\n",
    "        self.es = embed_size \n",
    "\n",
    "        self.wte = nn.Embedding(vocabulary_size, embed_size)\n",
    "        self.wpe = nn.Embedding(512, embed_size)\n",
    "\n",
    "        self.drop = nn.Dropout(.1)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerDecoderBlock(embed_size, intermediate_size, num_heads) for _ in range(hidden_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(embed_size, eps=1e-3)\n",
    "        self.lm_head = nn.Linear(embed_size, vocabulary_size)\n",
    "\n",
    "        self.position = torch.arange(512).unsqueeze(0)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids,\n",
    "            attention_mask = None,\n",
    "            k_cache = None,\n",
    "            v_cache = None,\n",
    "            start_cache = 0\n",
    "        ):\n",
    "        \n",
    "        if input_ids.device != self.position.device:\n",
    "            self.position = self.position.to(input_ids.device)\n",
    "        \n",
    "        B, L = input_ids.shape\n",
    "\n",
    "        pos_ids = self.position[:, :L]\n",
    "        token_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos_ids)\n",
    "\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for layer_id, block in enumerate(self.blocks):\n",
    "            x = block(\n",
    "                x,\n",
    "                attention_mask=attention_mask,\n",
    "                k_cache=k_cache,\n",
    "                v_cache=v_cache,\n",
    "                start_cache=start_cache,\n",
    "                layer=layer_id\n",
    "            )\n",
    "\n",
    "        output_embed = self.ln_f(x)\n",
    "        output_lm = self.lm_head(output_embed)\n",
    "\n",
    "        return output_embed, output_lm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d31f0",
   "metadata": {},
   "source": [
    "##  Training the transformer\n",
    "\n",
    "### I.Dataset \n",
    "For simplicity we will use a small dataset name TinyStories, we can load it using the huggingface Datasets library as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8315ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tinystories_dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "training_set = tinystories_dataset['train']\n",
    "print(training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626f71d",
   "metadata": {},
   "source": [
    "### The Dataloader\n",
    "\n",
    "We can define the batch_size default we will use, using the DataLoader object. Here we consider that the training machine will have enough RAM for a batch of 16 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e8bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dl = DataLoader(training_set, batch_size=16, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191921cb",
   "metadata": {},
   "source": [
    "### III. Tokenizer\n",
    "\n",
    "In this exercice we will consider the LLama Tokenizer, however, you can train your own tokenizer if you prefer (see lab 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd7f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32768\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model_hg_id = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_hg_id)\n",
    "tokenizer.pad_token = '<pad>'\n",
    "vocabulary_size = tokenizer.vocab_size\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bbecf6",
   "metadata": {},
   "source": [
    "### IV. Create the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc3d5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TransformerDecoder(vocabulary_size, 128, 256, 4, 2)\n",
    "model = model.train()\n",
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=6e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e635ccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (wte): Embedding(32768, 128)\n",
       "  (wpe): Embedding(512, 128)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-1): 2 x TransformerDecoderBlock(\n",
       "      (attention_module): RoPEAttentionModule(\n",
       "        (Wq): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (Wk): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (Wv): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (Wo): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): TransformerFeedForward(\n",
       "        (gate_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (up_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (down_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (gate_func): SiLU()\n",
       "      )\n",
       "      (attention_layer_norm): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (feed_forward_layer_norm): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=0.001, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=32768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f86e6",
   "metadata": {},
   "source": [
    "### VI. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ca404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at iteration 1 is 10.5585 remaining_time is 16:09:24s ( 2.3 it/s)\n",
      "The loss at iteration 501 is 4.8572 remaining_time is 06:27:13s ( 5.7 it/s)\n",
      "The loss at iteration 1001 is 3.5659 remaining_time is 06:20:07s ( 5.8 it/s)\n",
      "The loss at iteration 1501 is 3.2723 remaining_time is 06:15:47s ( 5.8 it/s)\n",
      "The loss at iteration 2001 is 3.0985 remaining_time is 06:14:12s ( 5.8 it/s)\n",
      "The loss at iteration 2501 is 2.9945 remaining_time is 06:13:01s ( 5.8 it/s)\n",
      "The loss at iteration 3001 is 2.8826 remaining_time is 06:12:00s ( 5.8 it/s)\n",
      "The loss at iteration 3501 is 2.8051 remaining_time is 06:10:26s ( 5.8 it/s)\n",
      "The loss at iteration 4001 is 2.7614 remaining_time is 06:08:14s ( 5.8 it/s)\n",
      "The loss at iteration 4501 is 2.7096 remaining_time is 06:06:13s ( 5.8 it/s)\n",
      "The loss at iteration 5001 is 2.6688 remaining_time is 06:04:21s ( 5.8 it/s)\n",
      "The loss at iteration 5501 is 2.6305 remaining_time is 06:03:20s ( 5.8 it/s)\n",
      "The loss at iteration 6001 is 2.5898 remaining_time is 06:01:33s ( 5.8 it/s)\n",
      "The loss at iteration 6501 is 2.5624 remaining_time is 06:00:06s ( 5.8 it/s)\n",
      "The loss at iteration 7001 is 2.5484 remaining_time is 05:58:10s ( 5.8 it/s)\n",
      "The loss at iteration 7501 is 2.5269 remaining_time is 05:57:18s ( 5.8 it/s)\n",
      "The loss at iteration 8001 is 2.4883 remaining_time is 05:55:41s ( 5.8 it/s)\n",
      "The loss at iteration 8501 is 2.4616 remaining_time is 05:54:19s ( 5.8 it/s)\n",
      "The loss at iteration 9001 is 2.4648 remaining_time is 05:53:28s ( 5.8 it/s)\n",
      "The loss at iteration 9501 is 2.4312 remaining_time is 05:52:08s ( 5.8 it/s)\n",
      "The loss at iteration 10001 is 2.4172 remaining_time is 05:50:33s ( 5.8 it/s)\n",
      "The loss at iteration 10501 is 2.3999 remaining_time is 05:49:15s ( 5.8 it/s)\n",
      "The loss at iteration 11001 is 2.3742 remaining_time is 05:47:38s ( 5.8 it/s)\n",
      "The loss at iteration 11501 is 2.3705 remaining_time is 05:46:26s ( 5.8 it/s)\n",
      "The loss at iteration 12001 is 2.3439 remaining_time is 05:45:08s ( 5.8 it/s)\n",
      "The loss at iteration 12501 is 2.3376 remaining_time is 05:44:05s ( 5.8 it/s)\n",
      "The loss at iteration 13001 is 2.3120 remaining_time is 05:43:19s ( 5.8 it/s)\n",
      "The loss at iteration 13501 is 2.2987 remaining_time is 05:42:05s ( 5.8 it/s)\n",
      "The loss at iteration 14001 is 2.2994 remaining_time is 05:41:05s ( 5.8 it/s)\n",
      "The loss at iteration 14501 is 2.2905 remaining_time is 05:39:38s ( 5.8 it/s)\n",
      "The loss at iteration 15001 is 2.2716 remaining_time is 05:37:53s ( 5.8 it/s)\n",
      "The loss at iteration 15501 is 2.2609 remaining_time is 05:36:43s ( 5.8 it/s)\n",
      "The loss at iteration 16001 is 2.2560 remaining_time is 05:35:59s ( 5.8 it/s)\n",
      "The loss at iteration 16501 is 2.2441 remaining_time is 05:34:49s ( 5.8 it/s)\n",
      "The loss at iteration 17001 is 2.2318 remaining_time is 05:33:38s ( 5.8 it/s)\n",
      "The loss at iteration 17501 is 2.2284 remaining_time is 05:32:10s ( 5.8 it/s)\n",
      "The loss at iteration 18001 is 2.2136 remaining_time is 05:30:51s ( 5.8 it/s)\n",
      "The loss at iteration 18501 is 2.2119 remaining_time is 05:29:25s ( 5.8 it/s)\n",
      "The loss at iteration 19001 is 2.2054 remaining_time is 05:27:58s ( 5.8 it/s)\n",
      "The loss at iteration 19501 is 2.1849 remaining_time is 05:26:40s ( 5.8 it/s)\n",
      "The loss at iteration 20001 is 2.1882 remaining_time is 05:25:24s ( 5.8 it/s)\n",
      "The loss at iteration 20501 is 2.1775 remaining_time is 05:24:12s ( 5.8 it/s)\n",
      "The loss at iteration 21001 is 2.1838 remaining_time is 05:22:52s ( 5.8 it/s)\n",
      "The loss at iteration 21501 is 2.1684 remaining_time is 05:21:21s ( 5.8 it/s)\n",
      "The loss at iteration 22001 is 2.1671 remaining_time is 05:19:47s ( 5.8 it/s)\n",
      "The loss at iteration 22501 is 2.1557 remaining_time is 05:18:24s ( 5.8 it/s)\n",
      "The loss at iteration 23001 is 2.1517 remaining_time is 05:17:01s ( 5.8 it/s)\n",
      "The loss at iteration 23501 is 2.1414 remaining_time is 05:15:24s ( 5.8 it/s)\n",
      "The loss at iteration 24001 is 2.1388 remaining_time is 05:13:45s ( 5.8 it/s)\n",
      "The loss at iteration 24501 is 2.1317 remaining_time is 05:12:15s ( 5.8 it/s)\n",
      "The loss at iteration 25001 is 2.1319 remaining_time is 05:10:45s ( 5.8 it/s)\n",
      "The loss at iteration 25501 is 2.1264 remaining_time is 05:09:06s ( 5.8 it/s)\n",
      "The loss at iteration 26001 is 2.1225 remaining_time is 05:07:25s ( 5.8 it/s)\n",
      "The loss at iteration 26501 is 2.1156 remaining_time is 05:05:51s ( 5.8 it/s)\n",
      "The loss at iteration 27001 is 2.1139 remaining_time is 05:04:12s ( 5.8 it/s)\n",
      "The loss at iteration 27501 is 2.1061 remaining_time is 05:02:35s ( 5.8 it/s)\n",
      "The loss at iteration 28001 is 2.1014 remaining_time is 05:01:03s ( 5.8 it/s)\n",
      "The loss at iteration 28501 is 2.1001 remaining_time is 04:59:30s ( 5.8 it/s)\n",
      "The loss at iteration 29001 is 2.0981 remaining_time is 04:57:58s ( 5.8 it/s)\n",
      "The loss at iteration 29501 is 2.0946 remaining_time is 04:56:24s ( 5.8 it/s)\n",
      "The loss at iteration 30001 is 2.0853 remaining_time is 04:54:47s ( 5.8 it/s)\n",
      "The loss at iteration 30501 is 2.0857 remaining_time is 04:53:14s ( 5.8 it/s)\n",
      "The loss at iteration 31001 is 2.0711 remaining_time is 04:51:40s ( 5.8 it/s)\n",
      "The loss at iteration 31501 is 2.0647 remaining_time is 04:50:10s ( 5.8 it/s)\n",
      "The loss at iteration 32001 is 2.0813 remaining_time is 04:48:39s ( 5.8 it/s)\n",
      "The loss at iteration 32501 is 2.0657 remaining_time is 04:47:05s ( 5.8 it/s)\n",
      "The loss at iteration 33001 is 2.0592 remaining_time is 04:45:31s ( 5.8 it/s)\n",
      "The loss at iteration 33501 is 2.0630 remaining_time is 04:45:19s ( 5.8 it/s)\n",
      "The loss at iteration 34001 is 2.0685 remaining_time is 04:45:13s ( 5.8 it/s)\n",
      "The loss at iteration 34501 is 2.0564 remaining_time is 04:45:02s ( 5.7 it/s)\n",
      "The loss at iteration 35001 is 2.0461 remaining_time is 04:44:41s ( 5.7 it/s)\n",
      "The loss at iteration 35501 is 2.0460 remaining_time is 04:44:20s ( 5.7 it/s)\n",
      "The loss at iteration 36001 is 2.0441 remaining_time is 05:34:10s ( 4.8 it/s)\n",
      "The loss at iteration 36501 is 2.0416 remaining_time is 05:32:18s ( 4.8 it/s)\n",
      "The loss at iteration 37001 is 2.0461 remaining_time is 05:30:23s ( 4.8 it/s)\n",
      "The loss at iteration 37501 is 2.0429 remaining_time is 05:27:35s ( 4.8 it/s)\n",
      "The loss at iteration 38001 is 2.0302 remaining_time is 05:24:46s ( 4.8 it/s)\n",
      "The loss at iteration 38501 is 2.0324 remaining_time is 05:21:59s ( 4.9 it/s)\n",
      "The loss at iteration 39001 is 2.0261 remaining_time is 05:19:14s ( 4.9 it/s)\n",
      "The loss at iteration 39501 is 2.0258 remaining_time is 05:16:40s ( 4.9 it/s)\n",
      "The loss at iteration 40001 is 2.0250 remaining_time is 05:14:01s ( 4.9 it/s)\n",
      "The loss at iteration 40501 is 2.0317 remaining_time is 05:11:30s ( 4.9 it/s)\n",
      "The loss at iteration 41001 is 2.0117 remaining_time is 05:09:00s ( 4.9 it/s)\n",
      "The loss at iteration 41501 is 2.0203 remaining_time is 05:06:34s ( 4.9 it/s)\n",
      "The loss at iteration 42001 is 2.0161 remaining_time is 05:04:07s ( 5.0 it/s)\n",
      "The loss at iteration 42501 is 2.0205 remaining_time is 05:01:42s ( 5.0 it/s)\n",
      "The loss at iteration 43001 is 2.0209 remaining_time is 04:59:19s ( 5.0 it/s)\n",
      "The loss at iteration 43501 is 2.0148 remaining_time is 04:56:59s ( 5.0 it/s)\n",
      "The loss at iteration 44001 is 2.0141 remaining_time is 04:54:44s ( 5.0 it/s)\n",
      "The loss at iteration 44501 is 2.0105 remaining_time is 04:52:23s ( 5.0 it/s)\n",
      "The loss at iteration 45001 is 2.0064 remaining_time is 04:49:59s ( 5.0 it/s)\n",
      "The loss at iteration 45501 is 2.0073 remaining_time is 04:47:46s ( 5.0 it/s)\n",
      "The loss at iteration 46001 is 1.9943 remaining_time is 04:45:34s ( 5.0 it/s)\n",
      "The loss at iteration 46501 is 2.0026 remaining_time is 04:43:26s ( 5.1 it/s)\n",
      "The loss at iteration 47001 is 2.0022 remaining_time is 04:41:19s ( 5.1 it/s)\n",
      "The loss at iteration 47501 is 1.9972 remaining_time is 04:39:12s ( 5.1 it/s)\n",
      "The loss at iteration 48001 is 1.9896 remaining_time is 04:37:09s ( 5.1 it/s)\n",
      "The loss at iteration 48501 is 1.9880 remaining_time is 04:35:03s ( 5.1 it/s)\n",
      "The loss at iteration 49001 is 1.9911 remaining_time is 04:32:59s ( 5.1 it/s)\n",
      "The loss at iteration 49501 is 1.9881 remaining_time is 04:30:57s ( 5.1 it/s)\n",
      "The loss at iteration 50001 is 1.9889 remaining_time is 04:28:55s ( 5.1 it/s)\n",
      "The loss at iteration 50501 is 1.9815 remaining_time is 04:26:56s ( 5.1 it/s)\n",
      "The loss at iteration 51001 is 1.9844 remaining_time is 04:24:56s ( 5.1 it/s)\n",
      "The loss at iteration 51501 is 1.9717 remaining_time is 04:22:56s ( 5.1 it/s)\n",
      "The loss at iteration 52001 is 1.9948 remaining_time is 04:20:55s ( 5.1 it/s)\n",
      "The loss at iteration 52501 is 1.9741 remaining_time is 04:18:57s ( 5.1 it/s)\n",
      "The loss at iteration 53001 is 1.9814 remaining_time is 04:16:59s ( 5.2 it/s)\n",
      "The loss at iteration 53501 is 1.9746 remaining_time is 04:15:03s ( 5.2 it/s)\n",
      "The loss at iteration 54001 is 1.9716 remaining_time is 04:13:07s ( 5.2 it/s)\n",
      "The loss at iteration 54501 is 1.9601 remaining_time is 04:11:12s ( 5.2 it/s)\n",
      "The loss at iteration 55001 is 1.9804 remaining_time is 04:09:22s ( 5.2 it/s)\n",
      "The loss at iteration 55501 is 1.9757 remaining_time is 04:07:32s ( 5.2 it/s)\n",
      "The loss at iteration 56001 is 1.9656 remaining_time is 04:05:43s ( 5.2 it/s)\n",
      "The loss at iteration 56501 is 1.9676 remaining_time is 04:03:55s ( 5.2 it/s)\n",
      "The loss at iteration 57001 is 1.9659 remaining_time is 04:02:04s ( 5.2 it/s)\n",
      "The loss at iteration 57501 is 1.9567 remaining_time is 04:00:12s ( 5.2 it/s)\n",
      "The loss at iteration 58001 is 1.9554 remaining_time is 03:58:20s ( 5.2 it/s)\n",
      "The loss at iteration 58501 is 1.9617 remaining_time is 03:56:32s ( 5.2 it/s)\n",
      "The loss at iteration 59001 is 1.9597 remaining_time is 03:54:44s ( 5.2 it/s)\n",
      "The loss at iteration 59501 is 1.9467 remaining_time is 03:52:56s ( 5.2 it/s)\n",
      "The loss at iteration 60001 is 1.9527 remaining_time is 03:51:09s ( 5.2 it/s)\n",
      "The loss at iteration 60501 is 1.9471 remaining_time is 03:49:23s ( 5.2 it/s)\n",
      "The loss at iteration 61001 is 1.9529 remaining_time is 03:47:36s ( 5.2 it/s)\n",
      "The loss at iteration 61501 is 1.9595 remaining_time is 03:45:50s ( 5.2 it/s)\n",
      "The loss at iteration 62001 is 1.9562 remaining_time is 03:44:04s ( 5.2 it/s)\n",
      "The loss at iteration 62501 is 1.9433 remaining_time is 03:42:18s ( 5.2 it/s)\n",
      "The loss at iteration 63001 is 1.9494 remaining_time is 03:40:29s ( 5.3 it/s)\n",
      "The loss at iteration 63501 is 1.9352 remaining_time is 03:38:43s ( 5.3 it/s)\n",
      "The loss at iteration 64001 is 1.9431 remaining_time is 03:36:57s ( 5.3 it/s)\n",
      "The loss at iteration 64501 is 1.9411 remaining_time is 03:35:13s ( 5.3 it/s)\n",
      "The loss at iteration 65001 is 1.9370 remaining_time is 03:33:28s ( 5.3 it/s)\n",
      "The loss at iteration 65501 is 1.9364 remaining_time is 03:31:44s ( 5.3 it/s)\n",
      "The loss at iteration 66001 is 1.9391 remaining_time is 03:30:00s ( 5.3 it/s)\n",
      "The loss at iteration 66501 is 1.9371 remaining_time is 03:28:16s ( 5.3 it/s)\n",
      "The loss at iteration 67001 is 1.9349 remaining_time is 03:26:29s ( 5.3 it/s)\n",
      "The loss at iteration 67501 is 1.9448 remaining_time is 03:24:42s ( 5.3 it/s)\n",
      "The loss at iteration 68001 is 1.9363 remaining_time is 03:22:57s ( 5.3 it/s)\n",
      "The loss at iteration 68501 is 1.9265 remaining_time is 03:21:13s ( 5.3 it/s)\n",
      "The loss at iteration 69001 is 1.9295 remaining_time is 03:19:28s ( 5.3 it/s)\n",
      "The loss at iteration 69501 is 1.9394 remaining_time is 03:17:44s ( 5.3 it/s)\n",
      "The loss at iteration 70001 is 1.9271 remaining_time is 03:16:01s ( 5.3 it/s)\n",
      "The loss at iteration 70501 is 1.9316 remaining_time is 03:14:17s ( 5.3 it/s)\n",
      "The loss at iteration 71001 is 1.9196 remaining_time is 03:12:33s ( 5.3 it/s)\n",
      "The loss at iteration 71501 is 1.9339 remaining_time is 03:10:49s ( 5.3 it/s)\n",
      "The loss at iteration 72001 is 1.9360 remaining_time is 03:09:07s ( 5.3 it/s)\n",
      "The loss at iteration 72501 is 1.9329 remaining_time is 03:07:25s ( 5.3 it/s)\n",
      "The loss at iteration 73001 is 1.9302 remaining_time is 03:05:43s ( 5.3 it/s)\n",
      "The loss at iteration 73501 is 1.9186 remaining_time is 03:04:01s ( 5.3 it/s)\n",
      "The loss at iteration 74001 is 1.9269 remaining_time is 03:02:20s ( 5.3 it/s)\n",
      "The loss at iteration 74501 is 1.9222 remaining_time is 03:00:37s ( 5.3 it/s)\n",
      "The loss at iteration 75001 is 1.9268 remaining_time is 02:58:56s ( 5.4 it/s)\n",
      "The loss at iteration 75501 is 1.9249 remaining_time is 02:57:15s ( 5.4 it/s)\n",
      "The loss at iteration 76001 is 1.9234 remaining_time is 02:55:34s ( 5.4 it/s)\n",
      "The loss at iteration 76501 is 1.9213 remaining_time is 02:53:54s ( 5.4 it/s)\n",
      "The loss at iteration 77001 is 1.9157 remaining_time is 02:52:14s ( 5.4 it/s)\n",
      "The loss at iteration 77501 is 1.9136 remaining_time is 02:50:34s ( 5.4 it/s)\n",
      "The loss at iteration 78001 is 1.9250 remaining_time is 02:48:55s ( 5.4 it/s)\n",
      "The loss at iteration 78501 is 1.9114 remaining_time is 02:47:14s ( 5.4 it/s)\n",
      "The loss at iteration 79001 is 1.9097 remaining_time is 02:45:34s ( 5.4 it/s)\n",
      "The loss at iteration 79501 is 1.9125 remaining_time is 02:43:55s ( 5.4 it/s)\n",
      "The loss at iteration 80001 is 1.9188 remaining_time is 02:42:16s ( 5.4 it/s)\n",
      "The loss at iteration 80501 is 1.8970 remaining_time is 02:40:38s ( 5.4 it/s)\n",
      "The loss at iteration 81001 is 1.9146 remaining_time is 02:39:00s ( 5.4 it/s)\n",
      "The loss at iteration 81501 is 1.9151 remaining_time is 02:37:22s ( 5.4 it/s)\n",
      "The loss at iteration 82001 is 1.9127 remaining_time is 02:35:45s ( 5.4 it/s)\n",
      "The loss at iteration 82501 is 1.9058 remaining_time is 02:34:07s ( 5.4 it/s)\n",
      "The loss at iteration 83001 is 1.9105 remaining_time is 02:32:30s ( 5.4 it/s)\n",
      "The loss at iteration 83501 is 1.9142 remaining_time is 02:30:52s ( 5.4 it/s)\n",
      "The loss at iteration 84001 is 1.9064 remaining_time is 02:29:16s ( 5.4 it/s)\n",
      "The loss at iteration 84501 is 1.8929 remaining_time is 02:27:38s ( 5.4 it/s)\n",
      "The loss at iteration 85001 is 1.9065 remaining_time is 02:26:01s ( 5.4 it/s)\n",
      "The loss at iteration 85501 is 1.8972 remaining_time is 02:24:24s ( 5.4 it/s)\n",
      "The loss at iteration 86001 is 1.8931 remaining_time is 02:22:46s ( 5.4 it/s)\n",
      "The loss at iteration 86501 is 1.9004 remaining_time is 02:21:09s ( 5.4 it/s)\n",
      "The loss at iteration 87001 is 1.9066 remaining_time is 02:19:33s ( 5.4 it/s)\n",
      "The loss at iteration 87501 is 1.8971 remaining_time is 02:17:56s ( 5.4 it/s)\n",
      "The loss at iteration 88001 is 1.8908 remaining_time is 02:16:20s ( 5.4 it/s)\n",
      "The loss at iteration 88501 is 1.9013 remaining_time is 02:14:44s ( 5.4 it/s)\n",
      "The loss at iteration 89001 is 1.8940 remaining_time is 02:13:08s ( 5.4 it/s)\n",
      "The loss at iteration 89501 is 1.8969 remaining_time is 02:11:33s ( 5.4 it/s)\n",
      "The loss at iteration 90001 is 1.9010 remaining_time is 02:09:57s ( 5.4 it/s)\n",
      "The loss at iteration 90501 is 1.8949 remaining_time is 02:08:22s ( 5.5 it/s)\n",
      "The loss at iteration 91001 is 1.8914 remaining_time is 02:06:46s ( 5.5 it/s)\n",
      "The loss at iteration 91501 is 1.8867 remaining_time is 02:05:11s ( 5.5 it/s)\n",
      "The loss at iteration 92001 is 1.8816 remaining_time is 02:03:36s ( 5.5 it/s)\n",
      "The loss at iteration 92501 is 1.8850 remaining_time is 02:02:01s ( 5.5 it/s)\n",
      "The loss at iteration 93001 is 1.8839 remaining_time is 02:00:27s ( 5.5 it/s)\n",
      "The loss at iteration 93501 is 1.8946 remaining_time is 01:58:52s ( 5.5 it/s)\n",
      "The loss at iteration 94001 is 1.8966 remaining_time is 01:57:17s ( 5.5 it/s)\n",
      "The loss at iteration 94501 is 1.8840 remaining_time is 01:55:42s ( 5.5 it/s)\n",
      "The loss at iteration 95001 is 1.8863 remaining_time is 01:54:08s ( 5.5 it/s)\n",
      "The loss at iteration 95501 is 1.8942 remaining_time is 01:52:33s ( 5.5 it/s)\n",
      "The loss at iteration 96001 is 1.8831 remaining_time is 01:50:59s ( 5.5 it/s)\n",
      "The loss at iteration 96501 is 1.8893 remaining_time is 01:49:25s ( 5.5 it/s)\n",
      "The loss at iteration 97001 is 1.8847 remaining_time is 01:47:51s ( 5.5 it/s)\n",
      "The loss at iteration 97501 is 1.8881 remaining_time is 01:46:18s ( 5.5 it/s)\n",
      "The loss at iteration 98001 is 1.8931 remaining_time is 01:44:44s ( 5.5 it/s)\n",
      "The loss at iteration 98501 is 1.8760 remaining_time is 01:43:10s ( 5.5 it/s)\n",
      "The loss at iteration 99001 is 1.8863 remaining_time is 01:41:37s ( 5.5 it/s)\n",
      "The loss at iteration 99501 is 1.8771 remaining_time is 01:40:04s ( 5.5 it/s)\n",
      "The loss at iteration 100001 is 1.8843 remaining_time is 01:38:30s ( 5.5 it/s)\n",
      "The loss at iteration 100501 is 1.8892 remaining_time is 01:36:57s ( 5.5 it/s)\n",
      "The loss at iteration 101001 is 1.8831 remaining_time is 01:35:24s ( 5.5 it/s)\n",
      "The loss at iteration 101501 is 1.8801 remaining_time is 01:33:51s ( 5.5 it/s)\n",
      "The loss at iteration 102001 is 1.8821 remaining_time is 01:32:18s ( 5.5 it/s)\n",
      "The loss at iteration 102501 is 1.8745 remaining_time is 01:30:45s ( 5.5 it/s)\n",
      "The loss at iteration 103001 is 1.8779 remaining_time is 01:29:12s ( 5.5 it/s)\n",
      "The loss at iteration 103501 is 1.8799 remaining_time is 01:27:39s ( 5.5 it/s)\n",
      "The loss at iteration 104001 is 1.8770 remaining_time is 01:26:06s ( 5.5 it/s)\n",
      "The loss at iteration 104501 is 1.8808 remaining_time is 01:24:34s ( 5.5 it/s)\n",
      "The loss at iteration 105001 is 1.8759 remaining_time is 01:23:01s ( 5.5 it/s)\n",
      "The loss at iteration 105501 is 1.8773 remaining_time is 01:21:29s ( 5.5 it/s)\n",
      "The loss at iteration 106001 is 1.8786 remaining_time is 01:19:57s ( 5.5 it/s)\n",
      "The loss at iteration 106501 is 1.8752 remaining_time is 01:18:24s ( 5.5 it/s)\n",
      "The loss at iteration 107001 is 1.8665 remaining_time is 01:16:52s ( 5.5 it/s)\n",
      "The loss at iteration 107501 is 1.8661 remaining_time is 01:15:20s ( 5.5 it/s)\n",
      "The loss at iteration 108001 is 1.8720 remaining_time is 01:13:48s ( 5.5 it/s)\n",
      "The loss at iteration 108501 is 1.8675 remaining_time is 01:12:16s ( 5.5 it/s)\n",
      "The loss at iteration 109001 is 1.8660 remaining_time is 01:10:44s ( 5.5 it/s)\n",
      "The loss at iteration 109501 is 1.8718 remaining_time is 01:09:12s ( 5.5 it/s)\n",
      "The loss at iteration 110001 is 1.8702 remaining_time is 01:07:41s ( 5.5 it/s)\n",
      "The loss at iteration 110501 is 1.8633 remaining_time is 01:06:09s ( 5.5 it/s)\n",
      "The loss at iteration 111001 is 1.8697 remaining_time is 01:04:38s ( 5.5 it/s)\n",
      "The loss at iteration 111501 is 1.8657 remaining_time is 01:03:06s ( 5.5 it/s)\n",
      "The loss at iteration 112001 is 1.8693 remaining_time is 01:01:35s ( 5.5 it/s)\n",
      "The loss at iteration 112501 is 1.8642 remaining_time is 01:00:04s ( 5.5 it/s)\n",
      "The loss at iteration 113001 is 1.8624 remaining_time is 00:58:32s ( 5.5 it/s)\n",
      "The loss at iteration 113501 is 1.8646 remaining_time is 00:57:01s ( 5.5 it/s)\n",
      "The loss at iteration 114001 is 1.8704 remaining_time is 00:55:30s ( 5.6 it/s)\n",
      "The loss at iteration 114501 is 1.8591 remaining_time is 00:53:58s ( 5.6 it/s)\n",
      "The loss at iteration 115001 is 1.8669 remaining_time is 00:52:27s ( 5.6 it/s)\n",
      "The loss at iteration 115501 is 1.8521 remaining_time is 00:50:56s ( 5.6 it/s)\n",
      "The loss at iteration 116001 is 1.8588 remaining_time is 00:49:25s ( 5.6 it/s)\n",
      "The loss at iteration 116501 is 1.8708 remaining_time is 00:47:55s ( 5.6 it/s)\n",
      "The loss at iteration 117001 is 1.8722 remaining_time is 00:46:24s ( 5.6 it/s)\n",
      "The loss at iteration 117501 is 1.8519 remaining_time is 00:44:53s ( 5.6 it/s)\n",
      "The loss at iteration 118001 is 1.8609 remaining_time is 00:43:22s ( 5.6 it/s)\n",
      "The loss at iteration 118501 is 1.8653 remaining_time is 00:41:52s ( 5.6 it/s)\n",
      "The loss at iteration 119001 is 1.8630 remaining_time is 00:40:21s ( 5.6 it/s)\n",
      "The loss at iteration 119501 is 1.8563 remaining_time is 00:38:51s ( 5.6 it/s)\n",
      "The loss at iteration 120001 is 1.8566 remaining_time is 00:37:20s ( 5.6 it/s)\n",
      "The loss at iteration 120501 is 1.8572 remaining_time is 00:35:50s ( 5.6 it/s)\n",
      "The loss at iteration 121001 is 1.8608 remaining_time is 00:34:20s ( 5.6 it/s)\n",
      "The loss at iteration 121501 is 1.8593 remaining_time is 00:32:49s ( 5.6 it/s)\n",
      "The loss at iteration 122001 is 1.8583 remaining_time is 00:31:19s ( 5.6 it/s)\n",
      "The loss at iteration 122501 is 1.8639 remaining_time is 00:29:49s ( 5.6 it/s)\n",
      "The loss at iteration 123001 is 1.8580 remaining_time is 00:28:19s ( 5.6 it/s)\n",
      "The loss at iteration 123501 is 1.8680 remaining_time is 00:26:49s ( 5.6 it/s)\n",
      "The loss at iteration 124001 is 1.8602 remaining_time is 00:25:19s ( 5.6 it/s)\n",
      "The loss at iteration 124501 is 1.8570 remaining_time is 00:23:49s ( 5.6 it/s)\n",
      "The loss at iteration 125001 is 1.8577 remaining_time is 00:22:19s ( 5.6 it/s)\n",
      "The loss at iteration 125501 is 1.8461 remaining_time is 00:20:49s ( 5.6 it/s)\n",
      "The loss at iteration 126001 is 1.8561 remaining_time is 00:19:19s ( 5.6 it/s)\n",
      "The loss at iteration 126501 is 1.8615 remaining_time is 00:17:50s ( 5.6 it/s)\n",
      "The loss at iteration 127001 is 1.8403 remaining_time is 00:16:20s ( 5.6 it/s)\n",
      "The loss at iteration 127501 is 1.8609 remaining_time is 00:14:50s ( 5.6 it/s)\n",
      "The loss at iteration 128001 is 1.8471 remaining_time is 00:13:21s ( 5.6 it/s)\n",
      "The loss at iteration 128501 is 1.8592 remaining_time is 00:11:51s ( 5.6 it/s)\n",
      "The loss at iteration 129001 is 1.8477 remaining_time is 00:10:22s ( 5.6 it/s)\n",
      "The loss at iteration 129501 is 1.8627 remaining_time is 00:08:52s ( 5.6 it/s)\n",
      "The loss at iteration 130001 is 1.8465 remaining_time is 00:07:23s ( 5.6 it/s)\n",
      "The loss at iteration 130501 is 1.8553 remaining_time is 00:05:53s ( 5.6 it/s)\n",
      "The loss at iteration 131001 is 1.8458 remaining_time is 00:04:24s ( 5.6 it/s)\n",
      "The loss at iteration 131501 is 1.8508 remaining_time is 00:02:55s ( 5.6 it/s)\n",
      "The loss at iteration 132001 is 1.8446 remaining_time is 00:01:26s ( 5.6 it/s)\n"
     ]
    }
   ],
   "source": [
    "model = model.train()\n",
    "avg_loss = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, data in enumerate(training_dl):\n",
    "    res = tokenizer(data['text'], return_tensors=\"pt\", padding=True, padding_side='right', max_length=128, truncation=True)\n",
    "    x = res.input_ids[:, :-1] + 0\n",
    "    y = res.input_ids[:, 1:] + 0\n",
    "    # attention_mask = (res.attention_mask == 0)\n",
    "    optimizer.zero_grad()\n",
    "    oe, oy = model(x.to(device))\n",
    "    loss = loss_function(oy.to(device).view(-1, vocabulary_size), y.to(device).view(-1))\n",
    "    loss.backward()\n",
    "    loss_value = loss.item()\n",
    "    avg_loss.append(loss_value)\n",
    "    if( i%500 == 0):\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = int((elapsed_time/(i+1)) * (len(training_dl) - i))\n",
    "        loop_sec = ((i+1)/elapsed_time)\n",
    "        print(f\"The loss at iteration {i+1} is {sum(avg_loss)/len(avg_loss):3.4f} remaining_time is {strftime('%H:%M:%S', gmtime(remaining_time))}s ({loop_sec:4.1f} it/s)\", flush=True)\n",
    "        avg_loss = []\n",
    "    if( i%1000 == 0):\n",
    "        torch.save(optimizer.state_dict() ,\"optimizer_state_dict_llama_mini.pth.temp\")\n",
    "        torch.save(model.state_dict() ,\"transformer_state_dict_llama_mini.pth.temp\")\n",
    "        shutil.copyfile(\"optimizer_state_dict_llama_mini.pth.temp\", \"optimizer_state_dict_llama_mini.pth\")\n",
    "        shutil.copyfile(\"transformer_state_dict_llama_mini.pth.temp\", \"transformer_state_dict_llama_mini.pth\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38645a",
   "metadata": {},
   "source": [
    "## Decoding time\n",
    "\n",
    "There is different way to decode, the simplest one is the greedy decoding method, the principle is to loop with the new token produced each time (complete `nonefficient_greedy_decoding`) and choosing the token the most likely. We can also store Keys and Values as shown in course that in practice only accellerate the generation but not change it (complete `greedy_decoding`). And finally we can also sampling new tokens (complete `sampling_decoding` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899b529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonefficient_greedy_decoding(model, x, max_new_tokens=64):\n",
    "    previous_gen = x\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            _, logits = model(previous_gen)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        previous_gen = torch.cat([previous_gen, next_token], dim=1)\n",
    "\n",
    "    return previous_gen\n",
    "\n",
    "\n",
    "def greedy_decoding(model, x, max_new_tokens=64, max_cache_size=512, tokenizer=None):\n",
    "    model.eval()\n",
    "\n",
    "    k_cache = [None] * model.nhl\n",
    "    v_cache = [None] * model.nhl\n",
    "    generated = x\n",
    "    start_cache = 0\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            _, logits = model(\n",
    "                generated[:, -1:],\n",
    "                k_cache=k_cache,\n",
    "                v_cache=v_cache,\n",
    "                start_cache=start_cache\n",
    "            )\n",
    "\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        start_cache += 1\n",
    "\n",
    "        if generated.shape[1] >= max_cache_size:\n",
    "            break\n",
    "\n",
    "    return generated\n",
    "\n",
    "def sampling_decoding(model, x, max_new_tokens=64, max_cache_size=512, tokenizer=None, temperature=.7):\n",
    "    model.eval()\n",
    "\n",
    "    k_cache = [None] * model.nhl\n",
    "    v_cache = [None] * model.nhl\n",
    "    generated = x\n",
    "    start_cache = 0\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            _, logits = model(\n",
    "                generated[:, -1:],\n",
    "                k_cache=k_cache,\n",
    "                v_cache=v_cache,\n",
    "                start_cache=start_cache\n",
    "            )\n",
    "\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        start_cache += 1\n",
    "\n",
    "        if generated.shape[1] >= max_cache_size:\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4f2087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoder(vocabulary_size, 128, 256, 4, 2)\n",
    "model.load_state_dict(torch.load(\"transformer_state_dict_llama_mini.pth\", map_location=\"cpu\"))\n",
    "model = model.eval()\n",
    "\n",
    "prompt = \"Alice\"\n",
    "tokenized = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "input_ids = tokenized.input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_nonefficient = nonefficient_greedy_decoding(\n",
    "        model, input_ids, max_new_tokens=64\n",
    "    )\n",
    "\n",
    "    out_greedy = greedy_decoding(\n",
    "        model, input_ids, max_new_tokens=64, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    out_sampling = sampling_decoding(\n",
    "        model, input_ids, max_new_tokens=64, tokenizer=tokenizer, temperature=0.8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bf6770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT:\n",
      "Alice\n",
      "\n",
      "================================================================================\n",
      "Non-efficient Greedy Decoding:\n",
      "Alice was a little girl who loved to play with her toys. One day, she was playing with her dolls when she saw a big, scary monster. She was scared and ran away.\n",
      "\n",
      "Alice was very scared and ran away. She ran after it, but it was too fast. She ran after it\n",
      "\n",
      "================================================================================\n",
      "Greedy Decoding with KV cache:\n",
      "Alice One Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once Once\n",
      "\n",
      "================================================================================\n",
      "Sampling Decoding (temperature = 0.8):\n",
      "Alice Alice One The Once One Once One John John John John John John John John John John One Once Once Little Little Little Boy One Jack One Once Once Once Once Maggie at Once One Lily Once It Onceatted Once \" Once Jim John John John John John John Once Once One Once Once One Once Once John Ted Ted Teded Once\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PROMPT:\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Non-efficient Greedy Decoding:\")\n",
    "print(tokenizer.decode(out_nonefficient[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Greedy Decoding with KV cache:\")\n",
    "print(tokenizer.decode(out_greedy[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sampling Decoding (temperature = 0.8):\")\n",
    "print(tokenizer.decode(out_sampling[0], skip_special_tokens=True))\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f063be3",
   "metadata": {},
   "source": [
    "## Evaluation ?\n",
    "\n",
    "It remains difficult to evaluate the model. You can here propose an evalaution based on perplexity using an other model, or comparing results to ground truth using the test (here only validation) set, but it will not be totally informative !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14e34481",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = tinystories_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.8354\n",
      "Validation perplexity: 6.27\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "validation_dl = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dl:\n",
    "        enc = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "        input_ids = enc.input_ids[:, :-1]\n",
    "        target_ids = enc.input_ids[:, 1:]\n",
    "\n",
    "        _, logits = model(input_ids)\n",
    "\n",
    "        loss = loss_function(\n",
    "            logits.reshape(-1, vocabulary_size),\n",
    "            target_ids.reshape(-1)\n",
    "        )\n",
    "\n",
    "        num_tokens = target_ids.numel()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "avg_loss = total_loss / total_tokens\n",
    "perplexity = math.exp(avg_loss)\n",
    "\n",
    "print(f\"Validation loss: {avg_loss:.4f}\")\n",
    "print(f\"Validation perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7de87d",
   "metadata": {},
   "source": [
    "## Perplexity-based Evaluation Results\n",
    "\n",
    "We evaluate the language model on the **validation split** of the TinyStories dataset using **cross-entropy loss** and **perplexity**.\n",
    "\n",
    "### Validation Results\n",
    "\n",
    "- **Validation Loss:** 1.8354  \n",
    "- **Validation Perplexity:** 6.27\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "A perplexity of **6.27** indicates that, on average, the model considers approximately **6 possible tokens** at each prediction step.  \n",
    "This reflects a **good predictive capability**, especially considering that:\n",
    "\n",
    "- the model was trained **from scratch**,\n",
    "- the architecture is **small-scale** (2 layers, 128 embedding size),\n",
    "- the dataset contains **simple but diverse narratives**.\n",
    "\n",
    "The relatively low validation loss confirms that the model has successfully learned:\n",
    "- basic syntactic structures,\n",
    "- common narrative patterns,\n",
    "- and short-term dependencies present in children stories.\n",
    "\n",
    "### Discussion and Limitations\n",
    "\n",
    "While perplexity provides a useful quantitative evaluation, it does not fully capture:\n",
    "- long-range coherence,\n",
    "- factual consistency,\n",
    "- or generation diversity.\n",
    "\n",
    "This is illustrated by the decoding experiments, where greedy decoding tends to produce repetitive outputs, while sampling introduces more diversity at the cost of coherence.\n",
    "\n",
    "Overall, these results validate the correctness of the implementation and the effectiveness of the training procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1750a02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
